\thispagestyle{fancy}
\chapter{Machine Learning modeller}
\label{chp:mlm}
I den tidligere resultat sektion under Matlab afsnittet blev der fremstillet en oversigt over de individuelle machine learning modellers præcision. Herfra fremgå det, at den mest præcise model er af typen ensemble learning og bliver af MATLAB kaldt Bagged Trees. Da denne model ifølge MATLAB virker særlig god til at klassificere netop EMG signaler optaget vha. Myo'en, vil det i det følgende blive forsøgt at give et overblik over modellen og den egenskaber, hvordan den er opbygget, samt hvordan den virker. 

I denne sammenhænge findes der en række andre modeller, der er tæt relateret til Bagged Trees modellen. Af disse vil Boosting efterfølgende blive gennemgået, da denne ligesom Bagged Trees stammer fra ensemble learning, og ligeledes er en af trænede modeller fra MATLAB. I forbindelse med Boosting vil egenskaber, opbygning og virkemåde blive gennemgået.

\section{Bagged Trees}
Som nævnt gav Bagged Trees modellen ifølge matlab det bedste klassifikationsresultat. \todola{inden præcision i procent} Ved er se nærmere på hvordan modellen er opbygget og hvordan den virker, vil det blive forsøget \todola{Hvordan skal formålet af dette afsnit skrives}at give et bedre indblik i, hvorfor modellens klassifikationspræcision er høj. 

\subsection{Ensemble}
Bagged Trees er som tidligere nævnt en metaalgoritme af ensemble learning. Metoden i ensemble learning er generelt set, at der ikke trænes over hele ens træningssæt, men kun trænes over et mindre subsæt af træningssættet. Herved laves forskellige regler for forskellige subsæt af data, typisk vha. simple learning metoder. Der genereres således regler, der giver gode resultater med høj klassifikationspræcision for mindre datasæt, men ikke nødvendigvis vil gøre det for større sæt af data. 
Det kan således siges at fordelen ved at behandle subsæt af træningssættet, frem for at se på træningssættet i sin helhed, er at det er lettere at finde simple regler for mindre datasæt.
\todola{Lav evt. billedeksempel med helt træningssæt og subsæt, med decision boundary}
\\Risikoen for klassifikationsreglernes lave nøjagtighed ved større datasæt har dog meget lille betydning, da alle regler for de enkelt subsæt kombineres ved ensemble learning. Således genereres der en kompleks regel med forventelig høj klassifikationspræcision for større datasæt, ud fra simple regler med høje præcision på små datasæt.
\\\\
I praksis bestå ensemble learning af tre dele:
\begin{enumerate}
\item Selektion af subset
\item Træning af regler på de udvalgte subset
\item Kombinering af regler 
\end{enumerate}

Her ligger forskellen mellem forskellige typer af ensemble learning i, hvordan de udvælger og behandler træningssættets subset, samt hvorledes de enkelte simple regler fra hvert subset tilslut kombineres til en mere kompleks regel. 
Her kommer bagging ind i billedet, da denne netop udvælger subsæt og kombinere regler på en bestemt måde. Dette vil blive uddybet i det følgende. 

\subsection{Bagging}
Ved bagging, som står for bootstrap aggregation anvendes subset, som ved andre ensemble metoder til at generere et antal learners, som kombineres for at forbedre nøjagtigheden ved genkendelsen. Ensemble learners adskiller sig ved måden de trænede hypoteser kombineres på, samt processen, hvormed de individuelle hypotesers subset genereres på. 

For bagging er metoden de forskellige subset genereres særlig interessant, da der anvendes her bootstrap sampling.
Ved bootstrap sampling, samples tilfældigt data til et nyt subset med ”replacement”, hvilket tillader data duplikation i de individuelle subset af træningssættet. I praksis betyder det, at når data samples fra træningssættet til et nyt subset fjernes den samplede data ikke fra det originale datasæt, men kan stadig blive stadig blive tilfældigt ”udvalgt” ved et efterfølgende sample. Havde man samplet uden replacement, ville den samplede data blive fjernet således, at data der allerede er repræsenteret i det aktuelle subset ikke vil kunne optræde flere gange således, at al data i det genererede subset vil være unikt.

En særlig egenskab ved bootstrap sampling er, at subset med samme størrelse, som det oprindelige dataset kan genereres med afvigelser fra originalen, hvilket er afgørende for modeltræning, hvor man ønsker at træne på subset af samme størrelse som det tilgængelige træningssæt. Det er således muligt er lave subset til træning med variationer, eftersom dele af dataet fra træningssættet er duplieringer og andre dele må blive undladt. 

Netop det faktum at dele af dataet ikke er med i alle subset gør bagging til et godt redskab til at undgå overfitting. Dette sker, da der er sandlighed for at dårlig data, som en simple learner under normale omstændigheder vil være følsom overfor, ikke vil optræde i alle subsets og derved ikke vil påvirke alle de generede learners. Når de forskellige learners kombineres vil den resulterende learner være mindre tilbøjelig til overfitting på den dårlige data, da de enkelte learners kun har set deres eget individuelle subset af data. På den måde den måde undgå modellen at blive vildledt af individuelle data punkter og hjælpes i højere grad til at finde strukturen i dataet, på samme måde som cross validation.

Kombineringen af de genererede learners er relativ simple for bagging. Ved klassifikation anvendes majorty voting, hvor klassifikationen som et flertal af de genererede classifiers har fundet korrekt, bliver outputtet fra den samlede classifier.
Ved regression tages blot gennemsnittet af de learners der er konstrueret ud fra træningssættets subsets. Ideen med bagging, at anvende simple unbiased learners med høj varians og få et output med høj nøjagtighed ved at træne mange forskellige learners på tilfældigt samplet subsets af træningssættet og lave midling over deres resultater. Her kan en hver arbitrær learner anvendes, men ofte anvendes decision tree learners sammen med bagging, hvilket ifølge MATLAB ligeledes trænede en meget nøjagtig model på den opsamlede EMG data. Denne learner vil i det følgende blive set nærmere på.

\subsection{Decision tree}
Decision Trees er en specifik model til classification og regression learning. Her opbygges ved træning en træstruktur af beslutninger med en root, nodes og leaves. I denne træstruktur repræsenterer træets root, den første beslutning, hvorefter træet splittes op til nye nodes eller leaves. Her vil en ny node repræsenterer en ny beslutning, der skal træffes og derved splitte træningssættet yderligere. Hvorimod et leaf repræsenterer et muligt output for modellen. 

Indsæt billede

Ved hver node behandles en feature fra træningssættet. I træningssættet for dette projekt behandles f.eks. EMG sensorernes middelværdi over et vindue ved træning. En specifik feature fra dette træningssæt, som vil blive splittet på ved nodes i et decision tree, vil f.eks. være middelværdien fra Myo’ens sensor 1 over vinduet. 
Antallet af mulige splits på en bestemt feature afhænger bl.a. af dens type. Er en feature af typen boolean, vil al information fra denne feature blive kendt ved en enkelt node, yderligere information, vil således ikke kunne opnås ved flere splits på den konkrete feature. Hvis en feature i modsætning er et realt tal, som middelværdien fra en af Myo’ens sensorer, vil det være muligt at opnå et information gain ved yderligere splits. Herved kan visse features blive behandlet af adskillige nodes dybere i træstrukturen.
Når nok information til klassificering er opnået ved en gennemgang af træstrukturens nodes, vil beslutningsprocessen ende i et leaf med et resulterende output. 

Træets struktur bliver ved træning opbygget efter den aktuelle data i træningssættet, hvilket kan gøres efter en række forskellige teknikker. Generelt forsøger disse at lave så simple decision trees som muligt. Dette kan gøres ved at starte med de beslutninger, der opdeler mest muligt data. Herved vil træets root, være den beslutning, der giver den største information gain i træet. Næste niveau vil give det næstbedste split osv., således reduceres træstrukturens størrelse og derved dens kompleksitet. 

Dog vil decision trees, på trods af denne fremgangmåde kunne lave overfitting, hvis der optræder fejl i dataet, der trænes på. Som ved andre former for machine learning vil træningsalgoritme konstrueres, således at overfitting reduceres ved den resultatende model. Ved decision trees kan overfitting ofte kunne ses visuelt ved, at modellens træstruktur er blevet for stor. 

Dette kan forklares ved at dele træstrukturen bliver opbygget til at passe på data med støj eller andre fejl i træningssættet. Dette kan således resultere i en række nodes, som vil bidrage til overfitting.

Ved decision trees anvendes prouning, som er en proces, hvor idéen er at fjerne de nodes i træet, der er konstrueret, for at modellen kan passe på individuelle data punkter med støj. Denne proces resulterer i et mere simpelt træer og reducerer modellens overfitting. 
En metode, at lave prouning på er at træne hele træet ud fra træningssættet og derefter testes det aktuelle decision tree mod modellens validation set efter fejl. Efterfølgende udføres en prouning af træet, hvorefter det nye træet valideres op mod modellens validation set og testes efter fejl. Hvis det trænede decision tree med prouning har færre fejl på modellens validation set, vil en nye iteration med prouning blive udført og testes med samme metode. Hvis det trænede decision tree med prouning, derimod har flere fejl på modellens validation set, og dermed generaliserer dårligere end det oprindelige decision tree, vil det oprindelige decision tree blive anvendt og ingen yderligere prouning blive foretaget. 

Decision trees anvendes i den specifikke ensemble model random forest. Dennes træningsalgoritme er, hvad der konkret bruges, når der trænes Bagged Trees, men MATLAB’s Classification learner applikation. I det følgende vil vi se nærmere på netop random forest metoden.

\subsection{Random forest}
MATLAB anvender random forest algoritmen ved træning af Bagged trees. Det kan være vanskelige at beskrive nøjagtigt, hvorfor en træningsmodel virker bedre end en anden, men efter at undersøgt de ovennævnte træningstyper, vil det dog blive forsøgt at give et bud på, hvorfor præcisionen for denne model er høj.\todola{Find igen en mere præcis formulering}

En Random forest learner har kvaliteterne fra ensemble learning gennem dens anvendelse af bagging, og genererer adskillige decision trees ud fra subset af træningssættet. Outputtet af denne learner bliver efter bagging metoden gennemsnittet af de individuelle træers output ved regression og resultatet af en majority voting af træerne ved klassifikation. \todola{Random forest 3 ref}

\todola{Indsæt billede af Random forest}

Ved at kombinere resultatet af de enkelte generede træer, reduceres effekten af decision trees' tilbøjelighed til overfitting. Videre stiger modellens nøjagtighed, når resultatet fra flere træer kombineres. Dette resulterer i, at modellens kompleksitet kan øges uden at valideringsfejl fra overfitting stiger tilsvarende. 
Ved andre learners vil øget kompleksitet medførere en højere grad af sensitivitet overfor individuelle data punkter i træningssættet, hvilket mindsker modellernes evne til at generaliserer på ukendt data og derved resulterer i problemer med overfitting.\todola{Random forest 8,9,10 ref} Her har Random forest modellen således en fordel, da dennes kompleksitet kan øges, ved at genererer flere decision trees, uden tilsvarende at hæve variansen, men stadig bibeholde en lav bias. \todola{Random forest 3 ref}

Dette er højest sandsynligt, hvad resulterer i Random forest modellen er i stand til at klassificerer poses ud fra EMG data med så høj præcision. \todola{Indsæt modellens præcision i procent}  

\todola{Skriv afrunding efter MATLAB afsnit er færdig, generelt har modellen god præcision, fordi man kan anvende komplekse træer, som bliver genereret vha. feature bagging (Random forest 13), og videre anvende ordinær bagging til at kombinerer dem til en model med lav bias og varians.}
